{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_RMZdYiPrAyN",
        "outputId": "77c1a06a-9a09-4211-e533-4e88d2f880bc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting newspaper3k\n",
            "  Downloading newspaper3k-0.2.8-py3-none-any.whl (211 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/211.1 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━\u001b[0m \u001b[32m174.1/211.1 kB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.1/211.1 kB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: beautifulsoup4>=4.4.1 in /usr/local/lib/python3.10/dist-packages (from newspaper3k) (4.12.3)\n",
            "Requirement already satisfied: Pillow>=3.3.0 in /usr/local/lib/python3.10/dist-packages (from newspaper3k) (9.4.0)\n",
            "Requirement already satisfied: PyYAML>=3.11 in /usr/local/lib/python3.10/dist-packages (from newspaper3k) (6.0.1)\n",
            "Collecting cssselect>=0.9.2 (from newspaper3k)\n",
            "  Downloading cssselect-1.2.0-py2.py3-none-any.whl (18 kB)\n",
            "Requirement already satisfied: lxml>=3.6.0 in /usr/local/lib/python3.10/dist-packages (from newspaper3k) (4.9.4)\n",
            "Requirement already satisfied: nltk>=3.2.1 in /usr/local/lib/python3.10/dist-packages (from newspaper3k) (3.8.1)\n",
            "Requirement already satisfied: requests>=2.10.0 in /usr/local/lib/python3.10/dist-packages (from newspaper3k) (2.31.0)\n",
            "Collecting feedparser>=5.2.1 (from newspaper3k)\n",
            "  Downloading feedparser-6.0.11-py3-none-any.whl (81 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m81.3/81.3 kB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting tldextract>=2.0.1 (from newspaper3k)\n",
            "  Downloading tldextract-5.1.1-py3-none-any.whl (97 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m97.7/97.7 kB\u001b[0m \u001b[31m8.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting feedfinder2>=0.0.4 (from newspaper3k)\n",
            "  Downloading feedfinder2-0.0.4.tar.gz (3.3 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting jieba3k>=0.35.1 (from newspaper3k)\n",
            "  Downloading jieba3k-0.35.1.zip (7.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.4/7.4 MB\u001b[0m \u001b[31m18.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: python-dateutil>=2.5.3 in /usr/local/lib/python3.10/dist-packages (from newspaper3k) (2.8.2)\n",
            "Collecting tinysegmenter==0.3 (from newspaper3k)\n",
            "  Downloading tinysegmenter-0.3.tar.gz (16 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4>=4.4.1->newspaper3k) (2.5)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from feedfinder2>=0.0.4->newspaper3k) (1.16.0)\n",
            "Collecting sgmllib3k (from feedparser>=5.2.1->newspaper3k)\n",
            "  Downloading sgmllib3k-1.0.0.tar.gz (5.8 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk>=3.2.1->newspaper3k) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk>=3.2.1->newspaper3k) (1.3.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk>=3.2.1->newspaper3k) (2023.12.25)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk>=3.2.1->newspaper3k) (4.66.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.10.0->newspaper3k) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.10.0->newspaper3k) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.10.0->newspaper3k) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.10.0->newspaper3k) (2024.2.2)\n",
            "Collecting requests-file>=1.4 (from tldextract>=2.0.1->newspaper3k)\n",
            "  Downloading requests_file-2.0.0-py2.py3-none-any.whl (4.2 kB)\n",
            "Requirement already satisfied: filelock>=3.0.8 in /usr/local/lib/python3.10/dist-packages (from tldextract>=2.0.1->newspaper3k) (3.13.1)\n",
            "Building wheels for collected packages: tinysegmenter, feedfinder2, jieba3k, sgmllib3k\n",
            "  Building wheel for tinysegmenter (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for tinysegmenter: filename=tinysegmenter-0.3-py3-none-any.whl size=13538 sha256=8221704d1d411d4985f544015c6d10b258349771776592762c264b15267d06a5\n",
            "  Stored in directory: /root/.cache/pip/wheels/c8/d6/6c/384f58df48c00b9a31d638005143b5b3ac62c3d25fb1447f23\n",
            "  Building wheel for feedfinder2 (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for feedfinder2: filename=feedfinder2-0.0.4-py3-none-any.whl size=3340 sha256=bdbbc06c238475097ce4a7b14962ff022e45be87d46ef139f185707888e12726\n",
            "  Stored in directory: /root/.cache/pip/wheels/97/02/e7/a1ff1760e12bdbaab0ac824fae5c1bc933e41c4ccd6a8f8edb\n",
            "  Building wheel for jieba3k (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for jieba3k: filename=jieba3k-0.35.1-py3-none-any.whl size=7398381 sha256=7a7fc3619ffa62244c81acd749bd1140c9d221f80eb54061525ba834274745b2\n",
            "  Stored in directory: /root/.cache/pip/wheels/7a/c4/0c/12a9a314ecac499456c4c3b2fcc2f635a3b45a39dfbd240299\n",
            "  Building wheel for sgmllib3k (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sgmllib3k: filename=sgmllib3k-1.0.0-py3-none-any.whl size=6048 sha256=e9b2ba7d5b4f139236f4071b5d5448fcada7dfd9d335856eb6a877037fd42550\n",
            "  Stored in directory: /root/.cache/pip/wheels/f0/69/93/a47e9d621be168e9e33c7ce60524393c0b92ae83cf6c6e89c5\n",
            "Successfully built tinysegmenter feedfinder2 jieba3k sgmllib3k\n",
            "Installing collected packages: tinysegmenter, sgmllib3k, jieba3k, feedparser, cssselect, requests-file, feedfinder2, tldextract, newspaper3k\n",
            "Successfully installed cssselect-1.2.0 feedfinder2-0.0.4 feedparser-6.0.11 jieba3k-0.35.1 newspaper3k-0.2.8 requests-file-2.0.0 sgmllib3k-1.0.0 tinysegmenter-0.3 tldextract-5.1.1\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (4.12.3)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4) (2.5)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (2.31.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests) (2024.2.2)\n"
          ]
        }
      ],
      "source": [
        "!pip install newspaper3k\n",
        "!pip install beautifulsoup4\n",
        "!pip install requests\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **GET URL**"
      ],
      "metadata": {
        "id": "7ed9a_rEuJJK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import csv\n",
        "\n",
        "def crawl_url(base_url, total_pages):\n",
        "    try:\n",
        "        with open('url_list.csv', mode='w', newline='') as csv_file:\n",
        "            fieldnames = ['URL']\n",
        "            writer = csv.DictWriter(csv_file, fieldnames=fieldnames)\n",
        "            writer.writeheader()\n",
        "\n",
        "            for page in range(70, total_pages + 1):\n",
        "                url = f'{base_url}&sortby=time&sorttime=0&page={page}'\n",
        "                response = requests.get(url)\n",
        "                response.raise_for_status()\n",
        "                html_content = response.text\n",
        "                soup = BeautifulSoup(html_content, 'html.parser')\n",
        "                list_berita_div = soup.find('div', class_='list media_rows list-berita')\n",
        "                if list_berita_div:\n",
        "                    article_links = list_berita_div.find_all('a', href=True)\n",
        "                    for article_link in article_links:\n",
        "                        url = article_link['href']\n",
        "                        if url.startswith('https://news.detik.com/berita/'):\n",
        "                            writer.writerow({'URL': url})\n",
        "\n",
        "    except requests.exceptions.RequestException as e:\n",
        "        print(\"Error during HTTP request:\", e)\n",
        "\n",
        "base_url = 'https://www.detik.com/search/searchall?query=kriminal&siteid=2'\n",
        "total_pages = 427 #427\n",
        "crawl_url(base_url, total_pages)\n",
        "\n"
      ],
      "metadata": {
        "id": "Tc6DHP8NycxQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **GET DETAIL ARTICLE**"
      ],
      "metadata": {
        "id": "rJ02L8ihuQob"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from newspaper import Article\n",
        "import pandas as pd\n",
        "import time\n",
        "import os\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "\n",
        "\n",
        "file_csv = '/content/url_list.csv'\n",
        "tmp_folder = '/content/tmp'  # Folder sementara untuk file yang sudah di-crawl\n",
        "\n",
        "# Buat folder sementara jika belum ada\n",
        "if not os.path.exists(tmp_folder):\n",
        "    os.makedirs(tmp_folder)\n",
        "\n",
        "def get_article_info(link):\n",
        "    article = Article(link, language='id')\n",
        "    article.download()\n",
        "    article.parse()\n",
        "    article.nlp()\n",
        "    return {\n",
        "        'Judul': article.title,\n",
        "        'Penulis': article.authors,\n",
        "        'Tanggal publikasi': article.publish_date,\n",
        "        'Teks berita': article.text,\n",
        "        'Ringkasan' : article.summary,\n",
        "    }\n",
        "\n",
        "df = pd.read_csv(file_csv)\n",
        "\n",
        "articles_data = []\n",
        "\n",
        "start_time = time.time()  # Waktu mulai\n",
        "\n",
        "for idx, link in enumerate(df['URL']):\n",
        "    try:\n",
        "        article_info = get_article_info(link)\n",
        "        articles_data.append(article_info)\n",
        "\n",
        "        # Menyimpan data sementara setiap 100 artikel\n",
        "        if (idx + 1) % 100 == 0:\n",
        "            tmp_file = os.path.join(tmp_folder, f'tmp_{idx + 1}.csv')\n",
        "            df_tmp = pd.DataFrame(articles_data)\n",
        "            df_tmp.to_csv(tmp_file, index=False)\n",
        "            print(f\"Data untuk {idx + 1} artikel tersimpan sementara di {tmp_file}\")\n",
        "\n",
        "            # Menjeda selama 5 detik sebelum melanjutkan\n",
        "            time.sleep(5)\n",
        "\n",
        "    except Exception as e:\n",
        "        print(\"Gagal mengambil informasi dari link:\", link)\n",
        "        print(\"Error:\", str(e))\n",
        "\n",
        "# Simpan data akhir dalam file CSV\n",
        "df_result = pd.DataFrame(articles_data)\n",
        "output_csv = '/content/raw_data.csv'\n",
        "df_result.to_csv(output_csv, index=False)\n",
        "\n",
        "end_time = time.time()  # Waktu berakhir\n",
        "running_time = end_time - start_time\n",
        "\n",
        "# Menghapus folder sementara setelah selesai\n",
        "for tmp_file in os.listdir(tmp_folder):\n",
        "    tmp_path = os.path.join(tmp_folder, tmp_file)\n",
        "    os.remove(tmp_path)\n",
        "os.rmdir(tmp_folder)\n",
        "\n",
        "\n",
        "print(\"Data berhasil disimpan di\", output_csv)\n",
        "print(\"Waktu yang diperlukan:\", running_time, \"detik\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EE8d4BQ912OQ",
        "outputId": "bc099fc5-621e-4e4a-fa01-cf9b8785ab94"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Gagal mengambil informasi dari link: https://news.detik.com/berita/d-7160447/tebas-tangan-korban-hingga-putus-anggota-geng-motor-di-cilegon-ditangkap\n",
            "Error: Article `download()` failed with HTTPSConnectionPool(host='news.detik.com', port=443): Read timed out. (read timeout=7) on URL https://news.detik.com/berita/d-7160447/tebas-tangan-korban-hingga-putus-anggota-geng-motor-di-cilegon-ditangkap\n",
            "Data untuk 100 artikel tersimpan sementara di /content/tmp/tmp_100.csv\n",
            "Data untuk 200 artikel tersimpan sementara di /content/tmp/tmp_200.csv\n",
            "Data untuk 300 artikel tersimpan sementara di /content/tmp/tmp_300.csv\n",
            "Data untuk 400 artikel tersimpan sementara di /content/tmp/tmp_400.csv\n",
            "Data untuk 500 artikel tersimpan sementara di /content/tmp/tmp_500.csv\n",
            "Data untuk 600 artikel tersimpan sementara di /content/tmp/tmp_600.csv\n",
            "Gagal mengambil informasi dari link: https://news.detik.com/berita/d-6916141/ketua-pengadilan-yang-digerebek-mertua-sedang-selingkuh-di-hotel-dipecat\n",
            "Error: Article `download()` failed with HTTPSConnectionPool(host='news.detik.com', port=443): Read timed out. (read timeout=7) on URL https://news.detik.com/berita/d-6916141/ketua-pengadilan-yang-digerebek-mertua-sedang-selingkuh-di-hotel-dipecat\n",
            "Data berhasil disimpan di /content/raw_data.csv\n",
            "Waktu yang diperlukan: 2154.6788108348846 detik\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from newspaper import Article\n",
        "article = Article('https://www.cnnindonesia.com/tv/20221208234440-404-884961/video-waspada-modus-baru-penipuan-online','id') # Indonesia\n",
        "article.download()\n",
        "article.parse()\n",
        "print('Judul:', article.title)\n",
        "print('Penulis:', article.authors)\n",
        "print('Tanggal publikasi:', article.publish_date)\n",
        "print('Teks berita:', article.text)\n",
        "print('Gambar utama:', article.top_image)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tn_KrApERN7W",
        "outputId": "1eef4354-89c3-486b-be43-ae5d5d1a1fff"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (4.11.2)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4) (2.4.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (2.27.1)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests) (1.26.16)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests) (2023.7.22)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests) (3.4)\n"
          ]
        }
      ]
    }
  ]
}